{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ics5110': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a3d8fb96efc099e25dc4d6a873b44c7302ca5a21722eb7d7c39b09db89d1143d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default token pattern from sklearn\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "ngram_range = (1, 3)\n",
    "min_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yelp_review_sentiment_2classes.tsv\", delimiter=\"\\t\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "source": [
    "## train_test_split Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.1):\n",
    "    total_data = len(X)\n",
    "    total_test = int(total_data * test_size)\n",
    "    sample_idx = np.arange(total_data)\n",
    "    np.random.shuffle(sample_idx)\n",
    "\n",
    "    X_train = X.iloc[sample_idx[total_test:]]\n",
    "    y_train = y.iloc[sample_idx[total_test:]]\n",
    "\n",
    "    X_test = X.iloc[sample_idx[:total_test]]\n",
    "    y_test = y.iloc[sample_idx[:total_test]]\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = split_data(df[\"text\"], df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((224981,), (24997,))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "source": [
    "## CountVectorizer Implementation\n",
    "\n",
    "Something weird happen with sklearn's CountVectorizer\n",
    "\n",
    "Result from CountVectorizer. \n",
    "'beer', 'beer copyright', 'burger', 'copyright'  \n",
    "6 4 5 5  \n",
    "\n",
    "The correct one should be  \n",
    "'beer', 'pizza', 'beer copyright', 'burger', 'copyright'  \n",
    "6 4 4 5 5  \n",
    "Pizza is missing from the transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english') + [\"-PRON-\", \"-pron-\", \"PRON\", \"pron\"]\n",
    "\n",
    "def preprocessing(text, ngram_range):\n",
    "    tokens = token_pattern.findall(text)\n",
    "    tokens = list(filter(lambda x: x not in en_stopwords, tokens))\n",
    "    tokens = build_ngrams(tokens, ngram_range)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_ngrams(text, ngram_range):\n",
    "    vocabs = []\n",
    "    join_space = \" \".join\n",
    "    for i in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        for j in range(len(text)):\n",
    "            if j + i <= len(text):\n",
    "                vocabs.append(join_space(text[j:j+i]))\n",
    "    return vocabs\n",
    "\n",
    "\n",
    "def build_vocab(doc, ngram_range, min_freq):\n",
    "    vocabs = {}\n",
    "    vocab_counts = defaultdict(int)\n",
    "    for text in tqdm(doc):\n",
    "        tokens = preprocessing(text, ngram_range)\n",
    "        for word in tokens:\n",
    "            vocab_counts[word] += 1\n",
    "\n",
    "    vocab_counts = dict(filter(lambda x: x[1] >= min_freq, vocab_counts.items()))\n",
    "    for idx, key in enumerate(vocab_counts.keys()):\n",
    "        vocabs[key] = idx\n",
    "    return vocabs, vocab_counts\n",
    "\n",
    "\n",
    "def build_count_vector(doc, vocabs):\n",
    "    data = []\n",
    "    indices = []\n",
    "    indptr = [0]\n",
    "    for text in tqdm(doc):\n",
    "        tokens = preprocessing(text, ngram_range)\n",
    "        feature_counts = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            if word in vocabs:\n",
    "                feature_counts[vocabs[word]] += 1\n",
    "        data.extend(feature_counts.values())\n",
    "        indices.extend(feature_counts.keys())\n",
    "        indptr.append(len(data))\n",
    "    \n",
    "    return csr_matrix((data, indices, indptr), shape=(len(doc), len(vocabs)), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 224981/224981 [02:33<00:00, 1470.24it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabs, vocab_counts = build_vocab(X_train, ngram_range, min_freq)"
   ]
  },
  {
   "source": [
    "## TfIdfVectorizer Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_vector(doc, vocabs):\n",
    "    term_vectors = build_count_vector(doc, vocabs)\n",
    "    document_vectors = np.bincount(term_vectors.indices, minlength=term_vectors.shape[1])\n",
    "    # Add smoothing for both numerator and denumerator to prevent division by 0 or having 0\n",
    "    # as the result\n",
    "    idf = (term_vectors.shape[0] + 1) / (document_vectors + 1)\n",
    "    idf = np.log(idf)\n",
    "    # The default action for csr_matrix multiplied by numpy vector is dot product, therefore\n",
    "    # we need to create a diagonal matrix first to simulate element by element multiplication\n",
    "    # without blowing up the memory\n",
    "    diag_idf = diags(idf, offsets=0, format=\"csr\")\n",
    "    return term_vectors * diag_idf"
   ]
  },
  {
   "source": [
    "## Normalizer Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    new_data = []\n",
    "    for x in X:\n",
    "        square_sum = 0\n",
    "        for i in x.data:\n",
    "            square_sum += i * i\n",
    "        norm = np.sqrt(square_sum)\n",
    "        new_data.extend((x.data / norm).tolist())\n",
    "    new_mat = csr_matrix((np.array(new_data), X.indices, X.indptr), dtype=np.float64)\n",
    "    return new_mat"
   ]
  },
  {
   "source": [
    "## Cross validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_division(a, b):\n",
    "    if b == 0:\n",
    "        return 0.\n",
    "    return a / b\n",
    "\n",
    "\n",
    "def confusion_matrix(true, pred):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for y, y_hat in zip(true, pred):\n",
    "        if y == 1 and y_hat == 1:\n",
    "            tp += 1\n",
    "        elif y == 0 and y_hat == 1:\n",
    "            fp += 1\n",
    "        elif y == 1 and y_hat == 0:\n",
    "            fn += 1\n",
    "        elif y == 0 and y_hat == 0:\n",
    "            tn += 1\n",
    "    return tp, fp, tn, fn\n",
    "\n",
    "\n",
    "def precision_micro(true, pred):\n",
    "    tp, fp, tn, fn = confusion_matrix(true, pred)\n",
    "    return save_division(tp, (tp + fp))\n",
    "\n",
    "\n",
    "def recall_micro(true, pred):\n",
    "    tp, fp, tn, fn = confusion_matrix(true, pred)\n",
    "    return save_division(tp, (tp + fn))\n",
    "\n",
    "\n",
    "def f1_micro(true, pred):\n",
    "    precision = precision_micro(true, pred)\n",
    "    recall = recall_micro(true, pred)\n",
    "    return save_division((2 * precision * recall), (precision + recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import delayed, Parallel\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "\n",
    "def cross_validate(model, X, y, folds=5):\n",
    "    fold_size = int(X.shape[0] / folds)\n",
    "    fold_ranges = [(i, i + fold_size) for i in range(0, X.shape[0], fold_size)]\n",
    "    scores = {\n",
    "        \"fit_time\": [],\n",
    "        \"score_time\": [],\n",
    "        \"test_f1_micro\": [],\n",
    "        \"train_f1_micro\": [],\n",
    "        \"test_precision_micro\": [],\n",
    "        \"train_precision_micro\": [],\n",
    "        \"test_recall_micro\": [],\n",
    "        \"train_recall_micro\": []\n",
    "    }\n",
    "\n",
    "    def _fit(start, end):\n",
    "        # Need to use vstack to deal with sparse matrix\n",
    "        X_train = vstack([X[0:start], X[end:]])\n",
    "        y_train = np.concatenate([y[0:start], y[end:]])\n",
    "\n",
    "        X_test = X[start:end]\n",
    "        y_test = y[start:end]\n",
    "\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        fit_time = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        preds_train = model.predict(X_train)\n",
    "        preds_test = model.predict(X_test)\n",
    "        score_time = time.time() - start_time\n",
    "\n",
    "        metrics = [precision_micro, recall_micro, f1_micro]\n",
    "        scores = []\n",
    "        for metric in metrics:\n",
    "            name = metric.__name__\n",
    "            scores.append({\n",
    "                f\"train_{name}\": metric(y_train, preds_train),\n",
    "                f\"test_{name}\": metric(y_test, preds_test)})\n",
    "        return fit_time, score_time, scores\n",
    "\n",
    "    # for start, end in fold_ranges:\n",
    "    #     score = _fit(start, end)\n",
    "    #     print(score)\n",
    "    fit_results = Parallel(n_jobs=-1)(delayed(_fit)(start, end) for start, end in fold_ranges)\n",
    "    \n",
    "    for res in fit_results:\n",
    "        scores[\"fit_time\"].append(res[0])\n",
    "        scores[\"score_time\"].append(res[1])\n",
    "        for score in res[2]:\n",
    "            for k, v in score.items():\n",
    "                scores[k].append(v)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "source": [
    "## OneHotEncoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = 0\n",
    "        self.idx_mapping = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        unique_classes = np.unique(X)\n",
    "        for unique_class in unique_classes:\n",
    "            self.idx_mapping[unique_class] = len(self.idx_mapping)\n",
    "        self.num_class = unique_classes.shape[0]\n",
    "\n",
    "    def transform(self, X):\n",
    "        idx = []\n",
    "        for row in X:\n",
    "            idx.append(self.idx_mapping[row])\n",
    "        idx = np.array(idx)\n",
    "        onehot = np.zeros((X.shape[0], self.num_class), dtype=np.int64)\n",
    "        onehot[np.arange(X.shape[0]), idx] = 1\n",
    "        return onehot\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        if not self.idx_mapping and self.num_class == 0:\n",
    "            self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = 0\n",
    "        self.idx_mapping = {}\n",
    "\n",
    "    def fit(self, X):\n",
    "        unique_classes = np.unique(X)\n",
    "        for unique_class in unique_classes:\n",
    "            self.idx_mapping[unique_class] = len(self.idx_mapping)\n",
    "        self.num_class = unique_classes.shape[0]\n",
    "\n",
    "    def transform(self, X):\n",
    "        label = np.zeros(X.shape[0], dtype=np.int64)\n",
    "        for idx, row in enumerate(X):\n",
    "            label[idx] = self.idx_mapping[row]\n",
    "        return label\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        if not self.idx_mapping and self.num_class == 0:\n",
    "            self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "source": [
    "## Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Ensure numerical stability\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_scores / exp_scores.sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    m = (x * y).sum(1)\n",
    "    ce = np.log(m)\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, w=None, b=None, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.label_bin = OneHotEncoder()\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {\"w\": self.w, \"b\": self.b, \"lr\": self.lr, \"epochs\": self.epochs}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(self.label_bin.fit_transform(y))\n",
    "        limit = 1./np.sqrt(X.shape[1])\n",
    "        self.w = np.random.uniform(low=-limit, high=limit, size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-limit, high=limit, size=(y.shape[1]))\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = (X.T.dot(z - y) / X.shape[0])\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        z = softmax(z)\n",
    "        return z\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassification(object):\n",
    "    def __init__(self):\n",
    "        self.label_bin = OneHotEncoder()\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
    "        return {}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        _y = np.asarray(self.label_bin.fit_transform(y))\n",
    "        num_class = np.unique(y).shape[0]\n",
    "\n",
    "        self.prior = np.zeros((num_class))\n",
    "        for i in range(num_class):\n",
    "            self.prior[i] = (y == 1).sum() / y.shape[0]\n",
    "        self.log_prior = np.log(self.prior)\n",
    "\n",
    "        sum_words = (_y.T @ X) + 1\n",
    "        total_words = sum_words.sum()\n",
    "        self.log_likelihood = np.log(sum_words) - np.log(total_words)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        llh = X @ self.log_likelihood.T\n",
    "        posterior = llh + self.log_prior\n",
    "        # Normalized by Z but in logarithmic form\n",
    "        posterior = posterior - logsumexp(posterior, axis=1).reshape(-1, 1)\n",
    "        return np.exp(posterior)\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = self.predict_proba(X)\n",
    "        return preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 224981/224981 [02:01<00:00, 1852.83it/s]\n",
      "Cross-validate SoftmaxRegression-build_count_vector-None-None\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'todense'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"<ipython-input-39-5276f2464722>\", line 28, in _fit\n  File \"<ipython-input-29-d60b601a74d3>\", line 29, in fit\nAttributeError: 'numpy.ndarray' object has no attribute 'todense'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-192d539920f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cross-validate {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-5276f2464722>\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(model, X, y, folds)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#     score = _fit(start, end)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m#     print(score)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mfit_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfold_ranges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfit_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'todense'"
     ]
    }
   ],
   "source": [
    "logistic = SoftmaxRegression(lr=0.5, epochs=500)\n",
    "nb = NaiveBayesClassification()\n",
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(y_train)\n",
    "\n",
    "combinations = [\n",
    "    (logistic, build_count_vector, None, None),\n",
    "    (logistic, build_tfidf_vector, None, None),\n",
    "    (logistic, build_count_vector, normalize, None),\n",
    "    (logistic, build_tfidf_vector, normalize, None),\n",
    "    # (logistic, build_count_vector, None, pca_cnt),\n",
    "    # (logistic, build_tfidf_vector, None, pca_tfidf),\n",
    "    (nb, build_count_vector, None, None)\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for comb in combinations:\n",
    "    model = comb[0]\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    featurizer = comb[1]\n",
    "    featurizer_name = featurizer.__name__\n",
    "    _X_train = featurizer(X_train, vocabs)\n",
    "    _y_train = label_enc.transform(y_train)\n",
    "    # _X_test = featurizer(X_test, vocabs)\n",
    "    # _y_test = label_enc.transform(y_test)\n",
    "    \n",
    "    normalizer_name = \"None\"\n",
    "    if comb[2]:\n",
    "        normalizer = comb[2]\n",
    "        normalizer_name = normalizer.__name__\n",
    "        _X_train = normalizer(_X_train)\n",
    "        # _X_test = normalizer.transform(_X_test)\n",
    "        \n",
    "    decomposer_name = \"None\"\n",
    "    if comb[3]:\n",
    "        decomposer = comb[3]\n",
    "        decomposer = comb[3]\n",
    "        decomposer_name = decomposer.__name__\n",
    "        _X_train = decomposer(_X_train)\n",
    "        # _X_test = decomposer(_X_test)\n",
    "        \n",
    "    metadata = model_name + \"-\" + featurizer_name + \"-\" + normalizer_name + \"-\" + decomposer_name\n",
    "    \n",
    "    print(\"Cross-validate {}\".format(metadata))\n",
    "    score = cross_validate(model, _X_train, _y_train, folds=5)\n",
    "    res[metadata] = score\n",
    "    print(metadata, score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}