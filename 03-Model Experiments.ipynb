{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yelp_review_sentiment_2classes.tsv\", delimiter=\"\\t\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Experiments with 4 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english') + [\"-pron-\", \"pron\"]\n",
    "\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3), min_df=10, stop_words=en_stopwords)\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), min_df=10, stop_words=en_stopwords)\n",
    "label_enc = LabelEncoder()\n",
    "label_bin = LabelBinarizer()\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akurniawan/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((198000, 230884), (198000, 230884))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = df.sample(220000)\n",
    "train, test = train_test_split(sample_data, test_size=0.1, shuffle=True)\n",
    "\n",
    "cnt_vec.fit(train[\"text\"])\n",
    "tfidf_vec.fit(train[\"text\"])\n",
    "label_enc.fit(df[\"sentiment\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data to train..\n",
      "Cross-validate LogisticRegression-CountVectorizer-None\n",
      "LogisticRegression-CountVectorizer-None {'fit_time': array([2447.32094288, 2435.27109909, 2447.24594998, 2434.61051106,\n",
      "       2369.61255503]), 'score_time': array([0.204386  , 0.3321619 , 0.20633531, 0.47214317, 0.40806508]), 'test_f1_micro': array([0.65      , 0.64941919, 0.65234848, 0.65017677, 0.64891414]), 'train_f1_micro': array([0.99838384, 0.99828283, 0.99840909, 0.99835859, 0.99834596]), 'test_precision_micro': array([0.65      , 0.64941919, 0.65234848, 0.65017677, 0.64891414]), 'train_precision_micro': array([0.99838384, 0.99828283, 0.99840909, 0.99835859, 0.99834596]), 'test_recall_micro': array([0.65      , 0.64941919, 0.65234848, 0.65017677, 0.64891414]), 'train_recall_micro': array([0.99838384, 0.99828283, 0.99840909, 0.99835859, 0.99834596])}\n",
      "Prepare data to train..\n",
      "Cross-validate LogisticRegression-TfidfVectorizer-None\n",
      "LogisticRegression-TfidfVectorizer-None {'fit_time': array([423.11136198, 432.02383995, 400.60890007, 429.76350117,\n",
      "       427.4641242 ]), 'score_time': array([0.14662004, 0.07674217, 0.20067501, 0.09830999, 0.12761974]), 'test_f1_micro': array([0.68409091, 0.68441919, 0.6885101 , 0.68242424, 0.68515152]), 'train_f1_micro': array([0.83314394, 0.834375  , 0.83322601, 0.83315025, 0.83306818]), 'test_precision_micro': array([0.68409091, 0.68441919, 0.6885101 , 0.68242424, 0.68515152]), 'train_precision_micro': array([0.83314394, 0.834375  , 0.83322601, 0.83315025, 0.83306818]), 'test_recall_micro': array([0.68409091, 0.68441919, 0.6885101 , 0.68242424, 0.68515152]), 'train_recall_micro': array([0.83314394, 0.834375  , 0.83322601, 0.83315025, 0.83306818])}\n",
      "Prepare data to train..\n",
      "Cross-validate LogisticRegression-CountVectorizer-Normalizer\n",
      "LogisticRegression-CountVectorizer-Normalizer {'fit_time': array([528.87411618, 499.01473713, 510.59268117, 551.90089798,\n",
      "       524.45800805]), 'score_time': array([0.10629296, 0.19608498, 0.14710593, 0.06768632, 0.12864184]), 'test_f1_micro': array([0.68078283, 0.68174242, 0.6839899 , 0.67939394, 0.68121212]), 'train_f1_micro': array([0.75397096, 0.75424874, 0.75301768, 0.75418561, 0.75397096]), 'test_precision_micro': array([0.68078283, 0.68174242, 0.6839899 , 0.67939394, 0.68121212]), 'train_precision_micro': array([0.75397096, 0.75424874, 0.75301768, 0.75418561, 0.75397096]), 'test_recall_micro': array([0.68078283, 0.68174242, 0.6839899 , 0.67939394, 0.68121212]), 'train_recall_micro': array([0.75397096, 0.75424874, 0.75301768, 0.75418561, 0.75397096])}\n",
      "Prepare data to train..\n",
      "Cross-validate LogisticRegression-TfidfVectorizer-Normalizer\n",
      "LogisticRegression-TfidfVectorizer-Normalizer {'fit_time': array([417.51948023, 415.45464206, 430.5539372 , 424.23150778,\n",
      "       439.92411089]), 'score_time': array([0.1404798 , 0.18105793, 0.10616183, 0.11697721, 0.08378792]), 'test_f1_micro': array([0.68411616, 0.68426768, 0.6885101 , 0.68244949, 0.68510101]), 'train_f1_micro': array([0.8333649 , 0.83457702, 0.83332071, 0.8332197 , 0.83320076]), 'test_precision_micro': array([0.68411616, 0.68426768, 0.6885101 , 0.68244949, 0.68510101]), 'train_precision_micro': array([0.8333649 , 0.83457702, 0.83332071, 0.8332197 , 0.83320076]), 'test_recall_micro': array([0.68411616, 0.68426768, 0.6885101 , 0.68244949, 0.68510101]), 'train_recall_micro': array([0.8333649 , 0.83457702, 0.83332071, 0.8332197 , 0.83320076])}\n",
      "Prepare data to train..\n",
      "Cross-validate MultinomialNB-CountVectorizer-None\n",
      "MultinomialNB-CountVectorizer-None {'fit_time': array([0.80194879, 0.79458213, 0.76680398, 0.77655101, 0.76670408]), 'score_time': array([0.14892912, 0.17452502, 0.17317295, 0.16017795, 0.16872406]), 'test_f1_micro': array([0.63015152, 0.63005051, 0.62863636, 0.62631313, 0.63272727]), 'train_f1_micro': array([0.7135101 , 0.71373106, 0.71272727, 0.71248106, 0.71256944]), 'test_precision_micro': array([0.63015152, 0.63005051, 0.62863636, 0.62631313, 0.63272727]), 'train_precision_micro': array([0.7135101 , 0.71373106, 0.71272727, 0.71248106, 0.71256944]), 'test_recall_micro': array([0.63015152, 0.63005051, 0.62863636, 0.62631313, 0.63272727]), 'train_recall_micro': array([0.7135101 , 0.71373106, 0.71272727, 0.71248106, 0.71256944])}\n",
      "Prepare data to train..\n",
      "Cross-validate RandomForestClassifier-CountVectorizer-None\n",
      "RandomForestClassifier-CountVectorizer-None {'fit_time': array([5274.174649  , 5273.97677898, 5268.79587317, 5262.67987704,\n",
      "       5268.52152705]), 'score_time': array([26.89758086, 27.06985092, 20.78207517, 10.31403208, 20.926615  ]), 'test_f1_micro': array([0.6135101 , 0.61277778, 0.61409091, 0.61280303, 0.61671717]), 'train_f1_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_precision_micro': array([0.6135101 , 0.61277778, 0.61409091, 0.61280303, 0.61671717]), 'train_precision_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_recall_micro': array([0.6135101 , 0.61277778, 0.61409091, 0.61280303, 0.61671717]), 'train_recall_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268])}\n",
      "Prepare data to train..\n",
      "Cross-validate RandomForestClassifier-TfidfVectorizer-None\n",
      "RandomForestClassifier-TfidfVectorizer-None {'fit_time': array([4857.32646918, 4849.94711185, 4843.58540702, 4842.46209216,\n",
      "       4842.18226314]), 'score_time': array([14.76049399, 14.940696  , 14.58631873, 14.71790576, 14.19580388]), 'test_f1_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_f1_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_precision_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_precision_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_recall_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_recall_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268])}\n",
      "Prepare data to train..\n",
      "Cross-validate RandomForestClassifier-CountVectorizer-Normalizer\n",
      "RandomForestClassifier-CountVectorizer-Normalizer {'fit_time': array([4831.97311425, 4814.08076429, 4799.42218518, 4799.9808588 ,\n",
      "       4800.31488299]), 'score_time': array([20.53565788, 17.49214387, 13.12945008, 13.45480919, 13.38710093]), 'test_f1_micro': array([0.61022727, 0.61080808, 0.60909091, 0.60540404, 0.60921717]), 'train_f1_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_precision_micro': array([0.61022727, 0.61080808, 0.60909091, 0.60540404, 0.60921717]), 'train_precision_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_recall_micro': array([0.61022727, 0.61080808, 0.60909091, 0.60540404, 0.60921717]), 'train_recall_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268])}\n",
      "Prepare data to train..\n",
      "Cross-validate RandomForestClassifier-TfidfVectorizer-Normalizer\n",
      "RandomForestClassifier-TfidfVectorizer-Normalizer {'fit_time': array([4863.17444706, 4844.59765792, 4845.14817691, 4848.89596701,\n",
      "       4842.29199719]), 'score_time': array([21.38997984, 14.55444026, 14.60203695, 13.6011169 , 11.52279615]), 'test_f1_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_f1_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_precision_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_precision_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268]), 'test_recall_micro': array([0.61169192, 0.61128788, 0.61015152, 0.60664141, 0.60772727]), 'train_recall_micro': array([0.99989899, 0.99988636, 0.99991162, 0.99991793, 0.99989268])}\n"
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(random_state=42, max_iter=100000)\n",
    "mnb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "\n",
    "combinations = [\n",
    "    (logistic, cnt_vec, None),\n",
    "    (logistic, tfidf_vec, None),\n",
    "    (logistic, cnt_vec, normalizer),\n",
    "    (logistic, tfidf_vec, normalizer),\n",
    "    (mnb, cnt_vec, None),\n",
    "    (rf, cnt_vec, None),\n",
    "    (rf, tfidf_vec, None),\n",
    "    (rf, cnt_vec, normalizer),\n",
    "    (rf, tfidf_vec, normalizer),\n",
    "]\n",
    "\n",
    "res = {}\n",
    "for comb in combinations:\n",
    "    model = comb[0]\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print(\"Prepare data to train..\")\n",
    "    featurizer = comb[1]\n",
    "    featurizer_name = type(featurizer).__name__\n",
    "    X_train = featurizer.transform(train[\"text\"])\n",
    "    y_train = label_enc.transform(train[\"sentiment\"])\n",
    "    X_test = featurizer.transform(test[\"text\"])\n",
    "    y_test = label_enc.transform(test[\"sentiment\"])\n",
    "    \n",
    "    normalizer_name = \"None\"\n",
    "    if comb[2]:\n",
    "        normalizer = comb[2]\n",
    "        normalizer_name = type(normalizer).__name__\n",
    "        X_train = normalizer.fit_transform(X_train)\n",
    "        X_test = normalizer.transform(X_test)\n",
    "        \n",
    "    metadata = model_name + \"-\" + featurizer_name + \"-\" + normalizer_name\n",
    "    \n",
    "    print(\"Cross-validate {}\".format(metadata))\n",
    "    score = cross_validate(model, X_train, y_train,\n",
    "                           scoring=[\"f1_micro\", \"precision_micro\", \"recall_micro\"],\n",
    "                           cv=5, n_jobs=-1, return_train_score=True)\n",
    "    res[metadata] = score\n",
    "    print(metadata, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(res, open(\"4class_result.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model Experiments with 2 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english') + [\"-PRON-\", \"-pron-\", \"PRON\", \"pron\"]\n",
    "\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3), min_df=10, stop_words=en_stopwords)\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), min_df=10, stop_words=en_stopwords)\n",
    "label_enc = LabelEncoder()\n",
    "label_bin = LabelBinarizer()\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a5387059b3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mcnt_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mlabel_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1163\u001b[0m         \"\"\"\n\u001b[1;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[1;32m   1199\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m                           dtype=self.dtype)\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/ics5110/lib/python3.8/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36msort_indices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m             _sparsetools.csr_sort_indices(len(self.indptr) - 1, self.indptr,\n\u001b[0m\u001b[1;32m   1141\u001b[0m                                           self.indices, self.data)\n\u001b[1;32m   1142\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_sorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def replace_sentiment(sentiment):\n",
    "#     if sentiment == \"quite good\":\n",
    "#         return \"good\"\n",
    "#     elif sentiment == \"kind of bad\":\n",
    "#         return \"bad\"\n",
    "#     return sentiment\n",
    "\n",
    "# sample_data = df.sample(220000)\n",
    "# df[\"sentiment\"] = df[\"sentiment\"].map(replace_sentiment)\n",
    "train, test = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "\n",
    "cnt_vec.fit(train[\"text\"])\n",
    "tfidf_vec.fit(train[\"text\"])\n",
    "label_enc.fit(df[\"sentiment\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(random_state=42, max_iter=100000)\n",
    "mnb = MultinomialNB()\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "\n",
    "combinations = [\n",
    "    (logistic, cnt_vec, None),\n",
    "    (logistic, tfidf_vec, None),\n",
    "    (logistic, cnt_vec, normalizer),\n",
    "    (logistic, tfidf_vec, normalizer),\n",
    "    (mnb, cnt_vec, None),\n",
    "    (rf, cnt_vec, None),\n",
    "    (rf, tfidf_vec, None),\n",
    "    (rf, cnt_vec, normalizer),\n",
    "    (rf, tfidf_vec, normalizer),\n",
    "]\n",
    "\n",
    "res2 = {}\n",
    "for comb in combinations:\n",
    "    model = comb[0]\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    print(\"Prepare data to train..\")\n",
    "    featurizer = comb[1]\n",
    "    featurizer_name = type(featurizer).__name__\n",
    "    X_train = featurizer.transform(train[\"text\"])\n",
    "    y_train = label_enc.transform(train[\"sentiment\"])\n",
    "    X_test = featurizer.transform(test[\"text\"])\n",
    "    y_test = label_enc.transform(test[\"sentiment\"])\n",
    "    \n",
    "    normalizer_name = \"None\"\n",
    "    if comb[2]:\n",
    "        normalizer = comb[2]\n",
    "        normalizer_name = type(normalizer).__name__\n",
    "        X_train = normalizer.fit_transform(X_train)\n",
    "        X_test = normalizer.transform(X_test)\n",
    "        \n",
    "    metadata = model_name + \"-\" + featurizer_name + \"-\" + normalizer_name\n",
    "    \n",
    "    print(\"Cross-validate {}\".format(metadata))\n",
    "    score = cross_validate(model, X_train, y_train,\n",
    "                           scoring=[\"f1_micro\", \"precision_micro\", \"recall_micro\"],\n",
    "                           cv=5, n_jobs=-1, return_train_score=True)\n",
    "    res2[metadata] = score\n",
    "    print(metadata, score)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
