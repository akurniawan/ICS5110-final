{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a3d8fb96efc099e25dc4d6a873b44c7302ca5a21722eb7d7c39b09db89d1143d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using default token pattern from sklearn\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "ngram_range = (1, 3)\n",
    "min_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"yelp_review_sentiment_2classes.tsv\", delimiter=\"\\t\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "source": [
    "## CountVectorizer Implementation\n",
    "\n",
    "Something weird happen with sklearn's CountVectorizer\n",
    "\n",
    "Result from CountVectorizer. \n",
    "'beer', 'beer copyright', 'burger', 'copyright'  \n",
    "6 4 5 5  \n",
    "\n",
    "The correct one should be  \n",
    "'beer', 'pizza', 'beer copyright', 'burger', 'copyright'  \n",
    "6 4 4 5 5  \n",
    "Pizza is missing from the transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords = stopwords.words('english') + [\"-PRON-\", \"-pron-\", \"PRON\", \"pron\"]\n",
    "\n",
    "def preprocessing(text, ngram_range):\n",
    "    tokens = token_pattern.findall(text)\n",
    "    tokens = list(filter(lambda x: x not in en_stopwords, tokens))\n",
    "    tokens = build_ngrams(tokens, ngram_range)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def build_ngrams(text, ngram_range):\n",
    "    vocabs = []\n",
    "    join_space = \" \".join\n",
    "    for i in range(ngram_range[0], ngram_range[1] + 1):\n",
    "        for j in range(len(text)):\n",
    "            if j + i <= len(text):\n",
    "                vocabs.append(join_space(text[j:j+i]))\n",
    "    return vocabs\n",
    "\n",
    "\n",
    "def build_vocab(doc, ngram_range, min_freq):\n",
    "    vocabs = {}\n",
    "    vocab_counts = defaultdict(int)\n",
    "    for text in tqdm(doc):\n",
    "        tokens = preprocessing(text, ngram_range)\n",
    "        for word in tokens:\n",
    "            vocab_counts[word] += 1\n",
    "\n",
    "    vocab_counts = dict(filter(lambda x: x[1] >= min_freq, vocab_counts.items()))\n",
    "    for idx, key in enumerate(vocab_counts.keys()):\n",
    "        vocabs[key] = idx\n",
    "    return vocabs, vocab_counts\n",
    "\n",
    "\n",
    "def build_count_vector(doc, vocabs):\n",
    "    data = []\n",
    "    indices = []\n",
    "    indptr = [0]\n",
    "    for text in tqdm(doc):\n",
    "        tokens = preprocessing(text, ngram_range)\n",
    "        feature_counts = defaultdict(int)\n",
    "        for word in tokens:\n",
    "            if word in vocabs:\n",
    "                feature_counts[vocabs[word]] += 1\n",
    "        data.extend(feature_counts.values())\n",
    "        indices.extend(feature_counts.keys())\n",
    "        indptr.append(len(data))\n",
    "    \n",
    "    return csr_matrix((data, indices, indptr), shape=(len(doc), len(vocabs)), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 249978/249978 [01:47<00:00, 2317.17it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabs, vocab_counts = build_vocab(df[\"text\"], ngram_range, min_freq)\n",
    "# vocabs, vocab_counts = build_vocab((\n",
    "#     \"the pizza pizza beer copyright\",\n",
    "#     \"the pizza burger beer copyright\",\n",
    "#     \"the the pizza beer beer copyright\",\n",
    "#     \"the burger beer beer copyright\",\n",
    "#     \"the coke burger coke copyright\",\n",
    "#     \"the coke burger burger\",\n",
    "# ), (1, 3), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 249978/249978 [02:00<00:00, 2081.78it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<249978x234784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18601591 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "build_count_vector(df[\"text\"], vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "234784"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "source": [
    "vocab_counts[\"fat boy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer(min_df=4, ngram_range=(1, 3),\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...])"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "# tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), min_df=10, stop_words=en_stopwords)\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3), min_df=4, stop_words=en_stopwords)\n",
    "cnt_vec.fit((\n",
    "    \"the pizza pizza beer copyright\",\n",
    "    \"the pizza burger beer copyright\",\n",
    "    \"the the pizza beer beer copyright\",\n",
    "    \"the burger beer beer copyright\",\n",
    "    \"the coke burger coke copyright\",\n",
    "    \"the coke burger burger\",\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['beer', 'beer copyright', 'burger', 'copyright']\n[6 4 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(cnt_vec.get_feature_names())\n",
    "print(cnt_vec.transform((\n",
    "    \"the pizza pizza beer copyright\",\n",
    "    \"the pizza burger beer copyright\",\n",
    "    \"the the pizza beer beer copyright\",\n",
    "    \"the burger beer beer copyright\",\n",
    "    \"the coke burger coke copyright\",\n",
    "    \"the coke burger burger\",\n",
    ")).toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<6x4 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "res = cnt_vec.transform((\n",
    "    \"the pizza pizza beer copyright\",\n",
    "    \"the pizza burger beer copyright\",\n",
    "    \"the the pizza beer beer copyright\",\n",
    "    \"the burger beer beer copyright\",\n",
    "    \"the coke burger coke copyright\",\n",
    "    \"the coke burger burger\",\n",
    "))\n",
    "res"
   ]
  },
  {
   "source": [
    "## TfIdfVectorizer Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_vector(doc, vocabs):\n",
    "    term_vectors = build_count_vector(doc, vocabs)\n",
    "    document_vectors = np.bincount(term_vectors.indices, minlength=term_vectors.shape[1])\n",
    "    # Add smoothing for both numerator and denumerator to prevent division by 0 or having 0\n",
    "    # as the result\n",
    "    idf = (term_vectors.shape[0] + 1) / (document_vectors + 1)\n",
    "    idf = np.log(idf)\n",
    "    # The default action for csr_matrix multiplied by numpy vector is dot product, therefore\n",
    "    # we need to create a diagonal matrix first to simulate element by element multiplication\n",
    "    # without blowing up the memory\n",
    "    diag_idf = diags(idf, offsets=0, format=\"csr\")\n",
    "    return term_vectors * diag_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 249978/249978 [02:01<00:00, 2059.15it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<249978x234784 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18601591 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "build_tfidf_vector(df[\"text\"], vocabs)"
   ]
  },
  {
   "source": [
    "## Normalizer Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    new_data = []\n",
    "    for x in X:\n",
    "        square_sum = 0\n",
    "        for i in x.data:\n",
    "            square_sum += i * i\n",
    "        norm = np.sqrt(square_sum)\n",
    "        new_data.extend((x.data / norm).tolist())\n",
    "    new_mat = csr_matrix((np.array(new_data), X.indices, X.indptr), dtype=np.float64)\n",
    "    return new_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<6x4 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 254
    }
   ],
   "source": [
    "normalize(res)"
   ]
  },
  {
   "source": [
    "## train_test_split Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.1):\n",
    "    total_data = len(X)\n",
    "    total_test = int(total_data * test_size)\n",
    "    sample_idx = np.arange(total_data)\n",
    "    np.random.shuffle(sample_idx)\n",
    "\n",
    "    X_train = X.iloc[sample_idx[total_test:]]\n",
    "    y_train = y.iloc[sample_idx[total_test:]]\n",
    "\n",
    "    X_test = X.iloc[sample_idx[:total_test]]\n",
    "    y_test = y.iloc[sample_idx[:total_test]]\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "source": [
    "## Cross validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  }
 ]
}